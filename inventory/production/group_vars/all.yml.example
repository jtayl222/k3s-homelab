---
# =============================================================================
# HOMELAB K3S CLUSTER CONFIGURATION
# High-Resource Cluster: 5 nodes, 36 CPU cores, ~250GB RAM
# =============================================================================

ansible_python_interpreter: /usr/bin/python3

# K3s and Kubeconfig settings
k3s_server_path: /etc/rancher/k3s/k3s.yaml

# =============================================================================
# CORE INFRASTRUCTURE
# =============================================================================

# K3s Configuration
k3s_version: v1.33.1+k3s1
k3s_state: present

# Kubeconfig Management
kubeconfig_dir: "{{ playbook_dir }}/../fetched_tokens"
kubeconfig_path: "{{ kubeconfig_dir }}/k3s-kubeconfig"
control_plane_host: "{{ groups['k3s_control_plane'][0] }}"
control_plane_ip: "{{ hostvars[groups['k3s_control_plane'][0]]['ansible_host'] }}"

# Network Configuration
k3s_pod_cidr: "10.42.0.0/16"
k3s_service_cidr: "10.43.0.0/16"
k3s_api_port: 6443

# Storage
global_storage_class: "nfs-shared"
nfs_server_ip: "{{ hostvars[groups['nfs_server'][0]]['ansible_host'] }}"
nfs_path: "/srv/nfs/kubernetes"
nfs_allowed_networks: "192.168.1.0/24"

# =============================================================================
# OPTIMIZED RESOURCE ALLOCATION FOR HIGH-RESOURCE HOMELAB
# =============================================================================

# MinIO - High-Performance Storage
minio_namespace: "minio"
minio_access_key: "minioadmin"
minio_secret_key: "minioadmin123"
minio_nodeport: 30900
minio_console_nodeport: 30901
minio_memory_request: "8Gi"      # ⬆️ Increased for better performance
minio_memory_limit: "16Gi"       # ⬆️ Generous limit for caching
minio_cpu_request: "4"           # ⬆️ More CPU for I/O operations  
minio_cpu_limit: "8"             # ⬆️ Can burst for heavy workloads
minio_storage_size: "1Ti"        # ⬆️ Increased storage for ML artifacts
minio_internal_endpoint: "http://minio.minio.svc.cluster.local:9000"

# Prometheus Stack - Enhanced Monitoring
prometheus_namespace: "monitoring"
prometheus_nodeport: 30090
prometheus_storage_size: "200Gi"  # ⬆️ More metrics storage
# Prometheus resource optimization handled by role defaults

# Grafana - High-Performance Dashboards  
grafana_nodeport: 30300
grafana_admin_user: "admin"
grafana_admin_password: "admin123"
grafana_memory_request: "4Gi"    # ⬆️ Better dashboard performance
grafana_memory_limit: "8Gi"      # ⬆️ Handle complex queries
grafana_cpu_request: "2"         # ⬆️ Responsive UI
grafana_cpu_limit: "4"           # ⬆️ Can handle heavy dashboard loads
grafana_storage_size: "100Gi"    # ⬆️ More dashboard/plugin storage

# MLflow - Enhanced ML Experiment Tracking
mlflow_namespace: "mlflow"
mlflow_nodeport: 30800
mlflow_image: "ghcr.io/mlflow/mlflow:v2.13.0"
mlflow_storage_size: "500Gi"     # ⬆️ Massive experiment storage
mlflow_memory_request: "8Gi"     # ⬆️ Better for large experiment loads
mlflow_memory_limit: "16Gi"      # ⬆️ Handle big model artifacts
mlflow_cpu_request: "4"          # ⬆️ Faster artifact processing
mlflow_cpu_limit: "8"            # ⬆️ Can handle concurrent experiments
mlflow_s3_bucket: "mlflow-artifacts"
mlflow_s3_endpoint: "http://minio.minio.svc.cluster.local:9000"
mlflow_s3_access_key: "{{ minio_access_key }}"
mlflow_s3_secret_key: "{{ minio_secret_key }}"

# Seldon Core - High-Performance Model Serving
seldon_namespace: "seldon-system"
seldon_chart_repo: "https://storage.googleapis.com/seldon-charts"
seldon_chart_repo_name: "seldon"
seldon_chart_name: "seldon-core-operator"
seldon_memory_request: "4Gi"     # ⬆️ Better model loading performance
seldon_memory_limit: "12Gi"      # ⬆️ Handle large models
seldon_cpu_request: "2"          # ⬆️ Responsive model serving
seldon_cpu_limit: "8"            # ⬆️ High-throughput inference
seldon_minio_bucket: "seldon-models"
seldon_enable_analytics: true

# JupyterHub - High-Performance Data Science Environment
jupyterhub_namespace: "jupyterhub"
jupyterhub_nodeport: 30888
jupyterhub_password: "mlops123"
# Enhanced user resources for heavy ML workloads
jupyterhub_user_memory_request: "4Gi"    # ⬆️ Better for ML libraries
jupyterhub_user_memory_limit: "16Gi"     # ⬆️ Handle large datasets
jupyterhub_user_cpu_request: "1.0"       # ⬆️ Responsive notebooks
jupyterhub_user_cpu_limit: "8.0"         # ⬆️ Can use full node for training
jupyterhub_storage_type: "dynamic"       # Persistent user storage
jupyterhub_storage_size: "50Gi"          # ⬆️ Generous user storage

# Argo CD - GitOps Platform
argocd_namespace: "argocd"
argocd_nodeport: 30080

# Argo Workflows - High-Performance Pipeline Execution
argowf_namespace: "argowf"
argowf_nodeport: 32746
argowf_username: "admin"
argowf_password: "mlopsadmin123"
# Enhanced resources for ML pipeline execution
argowf_memory_request: "2Gi"     # ⬆️ Better workflow orchestration
argowf_memory_limit: "8Gi"       # ⬆️ Handle complex pipelines
argowf_cpu_request: "1"          # ⬆️ Responsive workflow execution
argowf_cpu_limit: "4"            # ⬆️ Parallel workflow processing

# Kubernetes Dashboard
dashboard_nodeport: 30444

# =============================================================================
# ELASTICSEARCH STACK (OPTIONAL - HIGH RESOURCE USAGE)
# =============================================================================

# Elasticsearch - Disabled by default for homelab (uncomment if needed)
# elasticsearch_namespace: "elastic"
# elasticsearch_node_count: 2           # ⬇️ Reduced for homelab
# elasticsearch_memory_request: "8Gi"   # ⬇️ More reasonable for homelab
# elasticsearch_memory_limit: "16Gi"    # ⬇️ Still generous
# elasticsearch_cpu_request: "2"        # ⬇️ Adequate performance
# elasticsearch_cpu_limit: "4"          # ⬇️ Reasonable limit
# elasticsearch_storage_size: "200Gi"   # ⬇️ Sufficient for homelab

# =============================================================================
# SECURITY & NETWORKING
# =============================================================================

# Sealed Secrets
sealed_secrets_namespace: "kube-system"

# UFW Firewall
configure_ufw: true
ufw_default_policy_incoming: deny
ufw_default_policy_outgoing: allow

# =============================================================================
# FEATURE FLAGS FOR HOMELAB OPTIMIZATION
# =============================================================================

# Enable/Disable heavy components based on needs
enable_elasticsearch: false       # Disable by default (resource heavy)
enable_kubeflow: false           # Disable by default (very resource heavy)
enable_istio: false              # Disable by default (adds complexity)
enable_advanced_monitoring: true # Keep enhanced Prometheus/Grafana
enable_gpu_support: false       # Enable if you add GPU nodes

# Development vs Production modes
homelab_mode: true               # Enables homelab-specific optimizations
development_mode: true          # Relaxed resource constraints for experimentation
high_availability: false        # Single-instance deployments for homelab
